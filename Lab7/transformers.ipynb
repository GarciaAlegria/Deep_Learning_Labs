{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b79dbeac",
   "metadata": {},
   "source": [
    "# Laboratorio 7 Deep\n",
    "## Atención y transformadores\n",
    "### Abner Iván Garcia Alegria - 21285\n",
    "### Universidad del valle de Guatemala\n",
    "### Javier Fong\n",
    "### Deep Learning 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec8dc36e0473657d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:33.480262Z",
     "start_time": "2024-10-04T06:16:32.408536Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch #libreia de pytorch\n",
    "import torch.nn as nn #modulo de redes neuronales\n",
    "import math #modulo de matematicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f2a061",
   "metadata": {},
   "source": [
    "### Explicación de la clase LayerNormalization\n",
    "Esta clase es para la normalizacion de capas para un modelo de una red neuronal transformers, la normalizacion en capas nos ayuda a ajustar los valores de entrada para tener una media cercana un poco a 0 y la desviacion cercana a 1. Ahora para lo teorico Utilizamos normalizacion en la funcion forward para calcular la media y la desviacion estandar de x como vimos en la clase calculamos los valores de x en la ultima dimension y calculamos la desviacion de esos valores de x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d1f583d3356e6ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:36.794959Z",
     "start_time": "2024-10-04T06:16:36.790425Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clase para la normalizacion de capas para redes neuronales\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None: #Constructor de la clase LayerNormalization\n",
    "        super().__init__() #Inicializa la clase padre\n",
    "        self.eps = eps #epsilon es una constante que se le asigna a la clase\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha es un parametro que se le asigna a la clase\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias es un parametro que se le asigna a la clase\n",
    "\n",
    "    def forward(self, x): # funcion forward de la clase LayerNormalization\n",
    "        mean = x.mean(dim = -1, keepdim = True) # se calcula la media de x en la ultima dimension\n",
    "        std = x.std(dim = -1, keepdim = True)  # se calcula la desviacion estandar de x en la ultima dimension\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias # se retorna el valor normalizado de x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0147b695",
   "metadata": {},
   "source": [
    "### Explicación de la clase FeedForwardBlock\n",
    "Esta clase lo que hace es que procesa los datos de manera eficiente mediante un modelo en una red neuronal transformers lo cual crea 2 como capas lineales lo cual complementado con lo teorico estas capas toma la entrada de un vector de tamaño n y como que lo proyecta en un espacio de mayor tamaño, en cambio la segunda capa es como lo contrario al primero ya que este hace lo contrario es decir reduce las dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c41cd1eeba9f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:46.526151Z",
     "start_time": "2024-10-04T06:16:46.514438Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clase para la atencion multi-cabeza de las redes neuronales\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None: #Constructor de la clase FeedForwardBlock\n",
    "        super().__init__() #Inicializa la clase padre de FeedForwardBlock\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)  # se crea una capa lineal con d_model entradas y d_ff salidas\n",
    "        self.dropout = nn.Dropout(dropout) # se crea una capa de dropout con el valor de dropout asignado a la clase FeedForwardBlock\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)  # se crea una capa lineal con d_ff entradas y d_model salidas \n",
    "\n",
    "    def forward(self, x): # funcion forward de la clase FeedForwardBlock \n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x)))) # se retorna el valor de la segunda capa lineal despues de aplicarle la funcion de activacion relu y dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0954cfe",
   "metadata": {},
   "source": [
    "### Explicación de la clase InputEmbeddings\n",
    "Esta clase como bien su nombre lo dice es como insertar posiciones de entrada que son como palabras numericas el cual genera vectores podriamos decir para que un modelo pueda entender estas palabras dentro de un contexto es como decir que tenemos comentarios de personas random y queremos saber que sentimientos da estos comentarios el cual esta clase puede hacer eso ya que se encarga de ver estas palabras y determinar que sentimientos puede causar dicho comentario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daab70ff78271aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:50.226749Z",
     "start_time": "2024-10-04T06:16:50.221461Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clases para insertar la posicion de las palabras en la red neuronal\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None: #Constructor de la clase InputEmbeddings \n",
    "        super().__init__() #Inicializa la clase padre de InputEmbeddings\n",
    "        self.d_model = d_model # se le asigna el valor de d_model a la clase InputEmbeddings\n",
    "        self.vocab_size = vocab_size # se le asigna el valor de vocab_size a la clase InputEmbeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # se crea una capa de embedding con vocab_size entradas y d_model salidas \n",
    "\n",
    "    def forward(self, x): # funcion forward de la clase InputEmbeddings para insertar la posicion de las palabras\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # se retorna el valor de la capa de embedding multiplicado por la raiz cuadrada de d_model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3802c3cc",
   "metadata": {},
   "source": [
    "### Explicación de la clase PositionalEncoding\n",
    "Esta clase se utiliza para incorporar informacion posicional en las entradas de un modelo, en pocas palabras nos ayuda a codificar la posicion de palabras en una secuencia para que el modelo pueda tener en cuenta el orden de estas, algo teorico que podemos ver aqui es cuando se crean matrices de un tamaño en especifico ya que lo que vimos es que la longitud de secuencia por el numero de dimenciones del modelo es donde se almacenan las codificaciones que nos dan por lo cual es super interesante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7ab738d06eb466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:53.785818Z",
     "start_time": "2024-10-04T06:16:53.776620Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clase para codificar la posicion de las palabras en un modelo en la red neuronal\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None: #Constructor de la clase PositionalEncoding\n",
    "        super().__init__() #Inicializa la clase padre de PositionalEncoding\n",
    "        self.d_model = d_model # se le asigna el valor de d_model a la clase PositionalEncoding en pocas palabras es el numero de dimensiones del modelo\n",
    "        self.seq_len = seq_len # se le asigna el valor de seq_len a la clase PositionalEncoding en pocas palabras es el numero de palabras o la longitud en la secuencia\n",
    "        self.dropout = nn.Dropout(dropout) # se crea una capa de dropout con el valor de dropout asignado a la clase PositionalEncoding\n",
    "        \n",
    "        pe = torch.zeros(seq_len, d_model) # se crea un tensor de ceros con las dimensiones de seq_len y d_model o matriz de ceros de seq_len x d_model\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # se crea un tensor con los valores de 0 a seq_len en float y se le agrega una dimension\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # se crea un tensor con los valores de 0 a d_model en float y se le agrega una dimension\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # se asigna a la matriz pe en las columnas pares los valores de la funcion seno\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # se asigna a la matriz pe en las columnas impares los valores de la funcion coseno\n",
    "        pe = pe.unsqueeze(0)  # se le agrega una dimension a la matriz pe para que quede de 1 x seq_len x d_model \n",
    "        self.register_buffer('pe', pe) # se registra el tensor pe en la clase PositionalEncoding como buffer \n",
    "\n",
    "    def forward(self, x): # funcion forward de la clase PositionalEncoding para codificar la posicion de las palabras\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model) se le suma a x el valor de pe en las dimensiones de x\n",
    "        return self.dropout(x) # se retorna el valor de x despues de aplicarle el dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e3ad9",
   "metadata": {},
   "source": [
    "### Explicación de la clase ResidualConnection\n",
    "Como su nombre lo dice implementa una conexion residual por lo cual estas nos ayudan como a mejorar el flujo de la informacion a traves de una red algo interesante de esta que podemos ver es que posee una capa de normalizacion la cual como vimos anteriormente esta ayuda a que la media y la desviacion esten como en equilibrio recordando que la media debe estar en 0 y la desviacion en 1, esto viendo en lo teorico de la clase esto pues ayuda a la convergencia como del entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b390e443e3431509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:56.169338Z",
     "start_time": "2024-10-04T06:16:56.160187Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clase para la conexion residual en un modelo de las redes neuronales\n",
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None: #Constructor de la clase ResidualConnection\n",
    "            super().__init__() #Inicializa la clase padre de ResidualConnection\n",
    "            self.dropout = nn.Dropout(dropout) # se crea una capa de dropout con el valor de dropout asignado a la clase ResidualConnection\n",
    "            self.norm = LayerNormalization(features) # se crea una capa de normalizacion con el valor de features asignado a la clase ResidualConnection\n",
    "    \n",
    "        def forward(self, x, sublayer): # funcion forward de la clase ResidualConnection\n",
    "            return x + self.dropout(sublayer(self.norm(x))) # se retorna el valor de x mas el valor de la capa de dropout aplicada a la capa de normalizacion de x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524f8ee",
   "metadata": {},
   "source": [
    "### Explicación de la clase MultiHeadAttentionBlock\n",
    "Esta clase implementa un bloque con atencion multicabeza que es como una parte fundamentar de los transformadores, esta permite que el modelo pueda tener la atencion en diferentes partes de la entrada de manera simultanea como que mejorando la capacidad de capturar patrones complejos viendo la parte teorica por ejemplo al dividir en tareas o diferentes cabezas el modelo puede enfocarse en diferentes partes de la secuencia en pocas palabras en forma paralela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f60a40ee5a93f250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:58.474673Z",
     "start_time": "2024-10-04T06:16:58.461541Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clase para agregar un bloque de atencion multicabeza en un modelo de las redes neuronales transformadores\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None: #Constructor de la clase MultiHeadAttentionBlock\n",
    "        super().__init__() #Inicializa la clase padre de MultiHeadAttentionBlock\n",
    "        self.d_model = d_model  # se le asigna el valor de d_model a la clase MultiHeadAttentionBlock en pocas palabras es el numero de dimensiones del modelo\n",
    "        self.h = h # se le asigna el valor de h a la clase MultiHeadAttentionBlock en pocas palabras es el numero de cabezas\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\" # se verifica que d_model sea divisible por h\n",
    "\n",
    "        self.d_k = d_model // h  # se le asigna el valor de d_k a la clase MultiHeadAttentionBlock en pocas palabras es el numero de dimensiones de la cabeza\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # se crea una capa lineal con d_model entradas y d_model salidas\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # se crea una capa lineal con d_model entradas y d_model salidas \n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # se crea una capa lineal con d_model entradas y d_model salidas\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # se crea una capa lineal con d_model entradas y d_model salidas\n",
    "        self.dropout = nn.Dropout(dropout) # se crea una capa de dropout con el valor de dropout asignado a la clase MultiHeadAttentionBlock\n",
    "\n",
    "    @staticmethod # metodo estatico para la atencion multicabeza en un modelo de las redes neuronales transformadores\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout): # funcion de atencion multicabeza en un modelo de las redes neuronales transformadores\n",
    "        d_k = query.shape[-1] # se le asigna el valor de d_k a la variable d_k\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k) # se calcula el producto punto de query y key y se divide por la raiz cuadrada de d_k\n",
    "        if mask is not None: # si mask es diferente de None\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) # se llena la matriz attention_scores con -1e9 en las posiciones donde mask sea igual a 0\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # se aplica la funcion softmax a attention_scores en la ultima dimension\n",
    "        if dropout is not None: # si dropout es diferente de None\n",
    "            attention_scores = dropout(attention_scores) # se aplica el dropout a attention_scores\n",
    "        return (attention_scores @ value), attention_scores # se retorna el producto punto de attention_scores y value y attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask): # funcion forward de la clase MultiHeadAttentionBlock\n",
    "        query = self.w_q(q)  # se le asigna el valor de w_q a query\n",
    "        key = self.w_k(k)  # se le asigna el valor de w_k a key \n",
    "        value = self.w_v(v) # se le asigna el valor de w_v a value\n",
    "\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2) # se le da la forma a query para que quede de (batch, h, seq_len, d_k)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2) # se le da la forma a key para que quede de (batch, h, seq_len, d_k)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2) # se le da la forma a value para que quede de (batch, h, seq_len, d_k)\n",
    "\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout) # se le asigna el valor de la funcion de atencion a x y self.attention_scores\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k) # se le da la forma a x para que quede de (batch, seq_len, d_model)\n",
    "\n",
    "        return self.w_o(x) # se retorna el valor de w_o aplicado a x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf88c43",
   "metadata": {},
   "source": [
    "### Explicación de la clase EncoderBlock\n",
    "Lo que hace esta clase es como implementar un bloque de codificacion en un modelo de redes el cual nos ayuda en la codificacion de secuencias ya que nos permite procesar la entrada de manera paralela como se habia mencionado anteriormente es como un complemento de la clase anterior ya que es una parte clave en la ejecucion paralela y siempre prestando atencion a todas las partes de la secuencia, algo visto en teoria que se aplica en esta clase es la autoatencion ya que esta es o hace que un modelo preste atencion a diferentes partes de la secuencia de entrada sin importar su distancia entre estos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92cdb1da2c2c2920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:04.850156Z",
     "start_time": "2024-10-04T06:17:04.844375Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clase para agregar un bloque de codificacion en un modelo de las redes neuronales transformadores\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None: #Constructor de la clase EncoderBlock\n",
    "        super().__init__() #Inicializa la clase padre de EncoderBlock\n",
    "        self.self_attention_block = self_attention_block # se le asigna el valor de self_attention_block a la clase EncoderBlock\n",
    "        self.feed_forward_block = feed_forward_block # se le asigna el valor de feed_forward_block a la clase EncoderBlock\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)]) # se crea una lista de conexiones residuales con 2 elementos \n",
    "\n",
    "    def forward(self, x, src_mask): # funcion forward de la clase EncoderBlock\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask)) # se le asigna el valor de la primera conexion residual a x aplicando la atencion multicabeza en x con x y x y src_mask \n",
    "        x = self.residual_connections[1](x, self.feed_forward_block) # se le asigna el valor de la segunda conexion residual a x aplicando el feed forward block en x \n",
    "        return x # retorna el valor de x \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49823b00",
   "metadata": {},
   "source": [
    "### Explicación de la clase Encoder\n",
    "Esta clase es importante ya que toma una secuencia de entrada y la transforma como en una representacion digamos de alto nivel obviamente aplicando multiples capas de codificacion, entonces estas capas hacen que el modelo capture relaciones mas complejas dentro de una secuencia, tambien podemos observar que posee una capa de normalizacion lo cual ya habiamos hablado de ella que ayuda como a equilibrar la parte media y la desviacion como bien se vio en la parte teorica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52695c77abc1cbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:06.721751Z",
     "start_time": "2024-10-04T06:17:06.715885Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clase para implementar el encoder en un modelo de las redes neuronales transformadores\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None: #Constructor de la clase Encoder\n",
    "        super().__init__() #Inicializa la clase padre de Encoder\n",
    "        self.layers = layers # se le asigna el valor de layers a la clase Encoder en pocas palabras es el numero de capas\n",
    "        self.norm = LayerNormalization(features) # se crea una capa de normalizacion con el valor de features asignado a la clase Encoder \n",
    "\n",
    "    def forward(self, x, mask): # funcion forward de la clase Encoder\n",
    "        for layer in self.layers: # para cada capa en layers se le asigna el valor de x aplicando la capa y mask \n",
    "            x = layer(x, mask) # se le asigna el valor de x aplicando la capa y mask \n",
    "        return self.norm(x) # retorna el valor de x aplicando la capa de normalizacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7c2e4",
   "metadata": {},
   "source": [
    "### Explicación de la clase DecoderBlock\n",
    "Este implementa un bloque de decodificacion como su nombre lo dice en un modelo de redes, el cual esta encargado de procesar la salida generada por el codificador, el objetivo teorico importante son las caracteristicas que se le asignan, ya que este como numero de caracteristicas o digamos dimensiones de modelo identico al codificador pues este define el tamaño de los vectores de representacion que genera el decodificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c7ea861080729ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:16.343228Z",
     "start_time": "2024-10-04T06:17:16.337035Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clase para agregar un bloque de decodificacion en un modelo de las redes neuronales transformadores\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None: #Constructor de la clase DecoderBlock\n",
    "        super().__init__() #Inicializa la clase padre de DecoderBlock\n",
    "        self.self_attention_block = self_attention_block # se le asigna el valor de self_attention_block a la clase DecoderBlock\n",
    "        self.cross_attention_block = cross_attention_block # se le asigna el valor de cross_attention_block a la clase DecoderBlock \n",
    "        self.feed_forward_block = feed_forward_block # se le asigna el valor de feed_forward_block a la clase DecoderBlock \n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)]) # se crea una lista de conexiones residuales con 3 elementos \n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask): # funcion forward de la clase DecoderBlock\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask)) # se le asigna el valor de la primera conexion residual a x aplicando la atencion multicabeza en x con x y x y tgt_mask\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask)) # se le asigna el valor de la segunda conexion residual a x aplicando la atencion multicabeza en x con encoder_output y encoder_output y src_mask\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block) # se le asigna el valor de la tercera conexion residual a x aplicando el feed forward block en x\n",
    "        return x # retorna el valor de x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f25f6",
   "metadata": {},
   "source": [
    "### Explicación de la clase Decoder\n",
    "Esta clase lo que hace es implementar el decoder en un modelo de red neuronal, el cual toma tanto la secuencia que se genera como la secuencia objetiva generada el objetivo teorico de esta es que posee tipo entrada, proceso y salida como en los programas, en la entrada tenemos x, encoder_output, src_mask, tgt_mask, en el proceso solo va recorriendo la capas del decodificador para aplicar la atencion propia a esa capa y en la salida vemos que al igual que clases anteriores hay una capa de normalizacion igual como se habia explicado anteriormente sobre esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f02f0c0fc48cf9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:18.977768Z",
     "start_time": "2024-10-04T06:17:18.971636Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clase para implementar el decoder en un modelo de las redes neuronales transformadores\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None: #Constructor de la clase Decoder\n",
    "        super().__init__() #Inicializa la clase padre de Decoder\n",
    "        self.layers = layers # se le asigna el valor de layers a la clase Decoder en pocas palabras es el numero de capas\n",
    "        self.norm = LayerNormalization(features) # se crea una capa de normalizacion con el valor de features asignado a la clase Decoder\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask): # funcion forward de la clase Decoder\n",
    "        for layer in self.layers: # para cada capa en layers se le asigna el valor de x aplicando la capa y encoder_output, src_mask y tgt_mask\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask) # se le asigna el valor de x aplicando la capa y encoder_output, src_mask y tgt_mask\n",
    "        return self.norm(x) # retorna el valor de x aplicando la capa de normalizacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c148d4",
   "metadata": {},
   "source": [
    "### Explicación de la clase ProjectionLayer\n",
    "Como el nombre lo dice implementa una capa de proyeccion en un modelo esto como para mapear las salidas de una red hacia un espacio x digamos o podria ser una categoria, esta al igual que la anterior contiene entrada, proceso y salida el cual con el objetivo teorico es que si se aplicara una transformacion lineal esto convertiria cada vector de un tamaño en especifico un un vector de tamaño como dice la clase un tamño vocab_size lo cual esto ayuda a que se realice una prediccion sobre la posicion de esta secuencia en especifico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "455c85041a44b369",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:21.764822Z",
     "start_time": "2024-10-04T06:17:21.757239Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clase para agregar en un modelo la proyeccion de las capas de las redes neuronales\n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None: #Constructor de la clase ProjectionLayer\n",
    "        super().__init__() #Inicializa la clase padre de ProjectionLayer\n",
    "        self.proj = nn.Linear(d_model, vocab_size) # se crea una capa lineal con d_model entradas y vocab_size salidas\n",
    "\n",
    "    def forward(self, x) -> None: # funcion forward de la clase ProjectionLayer\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size) \n",
    "        return self.proj(x) # retorna el valor de x aplicando la capa lineal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bea9b",
   "metadata": {},
   "source": [
    "### Explicación de la clase Transformer\n",
    "Aqui viene la cereza del pastel, esta clase implementa el modelo completo de red neuronal transformers, el cual como podemos ver en la clase esta incluye fases como la codificacion y decodificacion en conjunto con un proceso de proyeccion de salidas, algo interesante que me di cuenta es que aqui aplica como las partes de las clase anteriores de encode, decode y project ya que el objetivo teorico para esta es que ambas como funciones son las mismas que las que vimos anteriomente, el cual cada una de ellas posee entradas proceso y salida entonces para esta clase tenemos 3 entradas, 3 proceso y 3 salidas interesante no? ahora entiendo porque es la cereza del pastel esta clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16bc6f102d1184f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:24.503055Z",
     "start_time": "2024-10-04T06:17:24.497246Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clase para implementar el modelo de las redes neuronales transformadores\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None: #Constructor de la clase Transformer\n",
    "        super().__init__() #Inicializa la clase padre de Transformer\n",
    "        self.encoder = encoder # se le asigna el valor de encoder a la clase Transformer\n",
    "        self.decoder = decoder # se le asigna el valor de decoder a la clase Transformer\n",
    "        self.src_embed = src_embed # se le asigna el valor de src_embed a la clase Transformer\n",
    "        self.tgt_embed = tgt_embed # se le asigna el valor de tgt_embed a la clase Transformer\n",
    "        self.src_pos = src_pos # se le asigna el valor de src_pos a la clase Transformer\n",
    "        self.tgt_pos = tgt_pos # se le asigna el valor de tgt_pos a la clase Transformer\n",
    "        self.projection_layer = projection_layer # se le asigna el valor de projection_layer a la clase Transformer\n",
    "\n",
    "    def encode(self, src, src_mask): # funcion para codificar en un modelo de las redes neuronales transformadores\n",
    "        src = self.src_embed(src) # se le asigna el valor de src_embed a src\n",
    "        src = self.src_pos(src) # se le asigna el valor de src_pos a src\n",
    "        return self.encoder(src, src_mask) # retorna el valor de src aplicando el encoder y src_mask\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor): # funcion para decodificar en un modelo de las redes neuronales transformadores\n",
    "        tgt = self.tgt_embed(tgt) # se le asigna el valor de tgt_embed a tgt\n",
    "        tgt = self.tgt_pos(tgt) # se le asigna el valor de tgt_pos a tgt\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask) # retorna el valor de tgt aplicando el decoder y encoder_output, src_mask y tgt_mask\n",
    "    \n",
    "    def project(self, x): # funcion para proyectar en un modelo de las redes neuronales transformadores\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x) # retorna el valor de x aplicando la capa de proyeccion\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59edcc1",
   "metadata": {},
   "source": [
    "### Explicación de la funcion build_transformer\n",
    "Esta funcion crea un modelo transformer personalizado para tareas de procesamiento de secuencias, como vimos anteriormente la traduccion de texto o generacion de secuencias, al ver esta funcion nos damos cuenta que llama todas las clases es super genial como manda a llamar todas las clases como el decodificador, codificador las proyecciones, etc. lo cual es impresionante, ahora para el objetivo vemos que esta posee parametros como por ejemplo el tamaño del vocabulario de la entrada, tambien el tamaño de la salida, la longitud de la secuencia de entrada y de la del destino, las dimensiones, el numero de capas y asi sucesivamente ahora vemos porque esta completo este modelo de transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:26.589430Z",
     "start_time": "2024-10-04T06:17:26.579319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funcion para construir un modelo de las redes neuronales transformadores\n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer: # funcion para construir un modelo de las redes neuronales transformadores\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size) # se crea una capa de embedding con d_model entradas y src_vocab_size salidas\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size) # se crea una capa de embedding con d_model entradas y tgt_vocab_size salidas\n",
    "\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # se crea una capa de codificacion de posicion con d_model, src_seq_len y dropout\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # se crea una capa de codificacion de posicion con d_model, tgt_seq_len y dropout\n",
    "    \n",
    "    encoder_blocks = [] # se crea una lista de bloques de codificacion\n",
    "    for _ in range(N): # para cada capa en N se crea un bloque de codificacion\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # se crea un bloque de atencion multicabeza con d_model, h y dropout\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # se crea un bloque de feed forward con d_model, d_ff y dropout\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout) # se crea un bloque de codificacion con d_model, encoder_self_attention_block, feed_forward_block y dropout\n",
    "        encoder_blocks.append(encoder_block) # se agrega el bloque de codificacion a la lista de bloques de codificacion\n",
    "\n",
    "    decoder_blocks = [] # se crea una lista de bloques de decodificacion \n",
    "    for _ in range(N): # para cada capa en N se crea un bloque de decodificacion\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # se crea un bloque de atencion multicabeza con d_model, h y dropout\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # se crea un bloque de atencion multicabeza con d_model, h y dropout\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # se crea un bloque de feed forward con d_model, d_ff y dropout\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout) # se crea un bloque de decodificacion con d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block y dropout\n",
    "        decoder_blocks.append(decoder_block) # se agrega el bloque de decodificacion a la lista de bloques de decodificacion\n",
    "    \n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks)) # se crea un encoder con d_model y nn.ModuleList(encoder_blocks)\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks)) # se crea un decoder con d_model y nn.ModuleList(decoder_blocks)\n",
    "    \n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # se crea una capa de proyeccion con d_model y tgt_vocab_size\n",
    "    \n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer) # se crea un modelo de las redes neuronales transformadores con encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos y projection_layer\n",
    "\n",
    "    for p in transformer.parameters(): # para cada parametro en transformer se inicializa con xavier_uniform\n",
    "        if p.dim() > 1: # si la dimension de p es mayor a 1\n",
    "            nn.init.xavier_uniform_(p) # se inicializa p con xavier_uniform\n",
    "    \n",
    "    return transformer # retorna el modelo de las redes neuronales transformadores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81a0af",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "1. Merritt, R. (2022, 19 abril). ¿Qué es un modelo transformer? | Blog de NVIDIA. Blog Oficial de NVIDIA Latino América. https://la.blogs.nvidia.com/blog/que-es-un-modelo-transformer/\n",
    "2. Giacaglia, G. (2024, 23 marzo). How Transformers Work - towards data science. Medium. https://towardsdatascience.com/transformers-141e32e69591\n",
    "3. Loss, A. (2024, 30 enero). From Neural Networks to Transformers: The Evolution of Machine Learning. DATAVERSITY. https://www.dataversity.net/from-neural-networks-to-transformers-the-evolution-of-machine-learning/\n",
    "4. Jacon, T., & Jacon, T. (2023, 20 diciembre). Codificador y decodificador de IA en el procesamiento del lenguaje natural - Planeta Chatbot. Planeta Chatbot - Comunidad de expertos en IA Conversacional. https://planetachatbot.com/codificador-y-decodificador-de-ia-en-el-procesamiento-del-lenguaje-natural/\n",
    "5. Qué son los Transformers en PLN: ventajas e inconvenientes. (s. f.). https://blog.pangeanic.com/es/que-son-los-transformers-en-pln\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
